{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# 1. Loading & Understanding the data",
      "metadata": {
        "id": "0P-q9lySG9-H"
      }
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('train.csv')\ndf.head()",
      "metadata": {
        "id": "BJCFvmICGolp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "df.shape",
      "metadata": {
        "id": "niWGwAz4Hmvo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "df.info()",
      "metadata": {
        "id": "9k-vWnWKH0aF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "df.describe()",
      "metadata": {
        "id": "hBs-IDUxH2t6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "# 2. Cleaning the data",
      "metadata": {
        "id": "PRLB3rVoIWME"
      }
    },
    {
      "cell_type": "code",
      "source": "data = df.copy()",
      "metadata": {
        "id": "BTva9uG2IbMD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "data.isna().sum()",
      "metadata": {
        "id": "Sh6CgqypIElF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "missing_percentage = (data.isnull().sum() / len(data)) * 100\nmissing_percentage",
      "metadata": {
        "id": "vntuyEZmJGml"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "data.dropna(inplace=True)",
      "metadata": {
        "id": "IWbuga-2I3I9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "data.isna().sum()",
      "metadata": {
        "id": "TZBgr1UEI9_T"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "data.pickup_datetime = pd.to_datetime(data.pickup_datetime)\ndata.dropoff_datetime = pd.to_datetime(data.dropoff_datetime)",
      "metadata": {
        "id": "jJprhsk8JCpj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "data[['pickup_datetime', 'dropoff_datetime']].info()",
      "metadata": {
        "id": "Brt0DFGpOZv9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "data.duplicated().sum()",
      "metadata": {
        "id": "Gym1f9zgOdhF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "data[data['trip_duration'] <= 0]",
      "metadata": {
        "id": "0EyAxryBOzmE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "data['passenger_count'].unique()\n",
      "metadata": {
        "id": "RuF8trC_PcV9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "data['trip_duration_minutes'] = (data['trip_duration'] / 60).astype(int)",
      "metadata": {
        "id": "vBMOq-IlQTkT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(10, 5))\nsns.boxplot(x=data['trip_duration_minutes'])\nplt.xlabel('Trip Duration (minutes)')\nplt.title('Boxplot of Trip Duration')\nplt.show()\n",
      "metadata": {
        "id": "4HRlfUc5P8pL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "data['trip_duration_minutes'].quantile([0.95, 0.99, 0.999])",
      "metadata": {
        "id": "hasqpw3DWtkD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "95% of trips are under 35 minutes → Most trips are short.  \n99% of trips are under 55 minutes → Almost all normal trips fit here.  \n99.9% of trips are under 1386 minutes (~23 hours!) → A few extreme outliers exist.",
      "metadata": {
        "id": "RCWNFUOdXoQq"
      }
    },
    {
      "cell_type": "code",
      "source": "data = data[data.trip_duration_minutes < 55]",
      "metadata": {
        "id": "6bOMDeGwWBhF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "data.shape",
      "metadata": {
        "id": "n4mRN2AMWDg8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(10, 5))\nsns.boxplot(x=data['trip_duration_minutes'])\nplt.title(\"Trip Duration After Outlier Removal\")\nplt.show()",
      "metadata": {
        "id": "cSaNZcZeYMmp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "# 3. Feature Engineering",
      "metadata": {
        "id": "z2IjeyhDd8Bf"
      }
    },
    {
      "cell_type": "code",
      "source": "data['pickup_hour'] = data.pickup_datetime.dt.hour\ndata['pickup_day'] = data.pickup_datetime.dt.day_of_week\ndata['pickup_day_of_year'] = data.pickup_datetime.dt.day_of_year",
      "metadata": {
        "id": "mgLWy1gfelOk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Create Distance Feature\nfrom geopy.distance import geodesic\n\ndef haversine_distance(row):\n    pickup = (row['pickup_latitude'], row['pickup_longitude'])\n    dropoff = (row['dropoff_latitude'], row['dropoff_longitude'])\n    return geodesic(pickup, dropoff).km  # Distance in km\n\ndata['trip_distance_km'] = data.apply(haversine_distance, axis=1)\n",
      "metadata": {
        "id": "vm30CMKVgRvr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "data['avg_speed_kmh'] = data.trip_distance_km / (data.trip_duration_minutes / 60)",
      "metadata": {
        "id": "9kfNjDsRgZmh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(10, 5))\nsns.boxplot(x=data['avg_speed_kmh'])\nplt.title(\"Average speed (kmh)\")\nplt.show()",
      "metadata": {
        "id": "8aehKRTLTmqw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "threshold = data['avg_speed_kmh'].quantile(0.95)\nthreshold",
      "metadata": {
        "id": "yb6YqcV9T_S-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "data = data[data['avg_speed_kmh'] < threshold]",
      "metadata": {
        "id": "3lu4PM9qU9YN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "data.columns",
      "metadata": {
        "id": "SY3ZO4ligpvR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "# 4. Data Visualization & Insights",
      "metadata": {
        "id": "aPr6D6hwiqNw"
      }
    },
    {
      "cell_type": "code",
      "source": "# Trip duration distribution\n\nplt.figure(figsize=(10, 5))\nsns.histplot(data['trip_duration_minutes'], bins=50, kde=True, color='royalblue')\n\nplt.xlabel(\"Trip Duration (minutes)\", fontsize=12)\nplt.ylabel(\"Frequency\", fontsize=12)\nplt.title(\"Distribution of Trip Duration\", fontsize=14)\nplt.xlim(0, data['trip_duration_minutes'].quantile(0.99))  # Focus on 99% of data (remove extreme outliers)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\nplt.show()",
      "metadata": {
        "id": "uGy79ZwxisQn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Trip duration vs pickup hour\n\nplt.figure(figsize=(10, 6))\nsns.lineplot(x='pickup_hour', y='trip_duration_minutes', data=data, marker=\"o\", color=\"b\")\nplt.xlabel(\"Pickup Hour\", fontsize=12)\nplt.ylabel(\"Trip Duration (minutes)\", fontsize=12)\nplt.title(\"Trip Duration vs. Pickup Hour\", fontsize=14)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\nplt.show()",
      "metadata": {
        "id": "0iT9onHlkWWG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "numeric_data = data.select_dtypes(include=['number'])\ncorr_matrix = numeric_data.corr()\n\n# Plot the heatmap\nplt.figure(figsize=(10, 6))\nsns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5)\n\nplt.title(\"Correlation Heatmap of Numerical Features\", fontsize=14)\nplt.show()",
      "metadata": {
        "id": "NkzzAy1fm2WD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "id": "ykgaT7m_oqFB"
      }
    },
    {
      "cell_type": "markdown",
      "source": "# 5. Pre-Modeling",
      "metadata": {
        "id": "9NK6kzW_ossh"
      }
    },
    {
      "cell_type": "code",
      "source": "from sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# Define features (X) and target (y)\nX = data.drop(['trip_duration','trip_duration_minutes', 'id', 'pickup_datetime', 'dropoff_datetime'], axis=1)\ny = data['trip_duration_minutes']\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    random_state=42,\n    test_size=0.2\n)\n\n# Convert categorical columns to string (optional, to prevent dtype issues)\nX_train['vendor_id'] = X_train['vendor_id'].astype(str)\nX_test['vendor_id'] = X_test['vendor_id'].astype(str)\n\n# Encode 'vendor_id' and 'store_and_fwd_flag' separately\nle_vendor = LabelEncoder()\nX_train['vendor_id'] = le_vendor.fit_transform(X_train['vendor_id'])\nX_test['vendor_id'] = le_vendor.transform(X_test['vendor_id'])\n\nle_store = LabelEncoder()\nX_train['store_and_fwd_flag'] = le_store.fit_transform(X_train['store_and_fwd_flag'])\nX_test['store_and_fwd_flag'] = le_store.transform(X_test['store_and_fwd_flag'])\n\n# Scale features (Don't scale y)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n",
      "metadata": {
        "id": "kSGcuHjRo5mv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "print(X_train.isin([np.inf, -np.inf]).sum())  # Count infinities\nprint(X_train.isna().sum())  # Count NaNs\n\nprint(X_test.isin([np.inf, -np.inf]).sum())  # Count infinities\nprint(X_test.isna().sum())  # Count NaNs",
      "metadata": {
        "id": "-rYqw6beQrgr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "print(\"Mean:\", np.mean(X_train_scaled))\nprint(\"Std Dev:\", np.std(X_train_scaled))",
      "metadata": {
        "id": "rUXKrlg5VsFt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "# 5. Model Training & Evaluation",
      "metadata": {
        "id": "eLMz7-y_WESB"
      }
    },
    {
      "cell_type": "code",
      "source": "from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nmodels = {\n    'Linear Regression': LinearRegression(),\n    'Decision Tree': DecisionTreeRegressor(random_state=42),\n    'Random Forest': RandomForestRegressor(random_state=42)\n}\n\n# Perform cross-validation for each model\nfor name, model in models.items():\n    scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')\n    print(f\"{name} Cross-Validation R² Scores: {scores}\")\n    print(f\"Mean R² Score: {scores.mean():.4f}\")",
      "metadata": {
        "id": "4aca159f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "from sklearn.model_selection import GridSearchCV\n\n# Hyperparameter tuning for RandomForestRegressor\nparam_grid_rf = {'n_estimators': [50, 100, 200], 'max_depth': [5, 10, 20]}\ngrid_search_rf = GridSearchCV(RandomForestRegressor(random_state=42), param_grid_rf, cv=5, scoring='r2', n_jobs=-1)\ngrid_search_rf.fit(X_train_scaled, y_train)\nprint(\"Best Random Forest Parameters:\", grid_search_rf.best_params_)",
      "metadata": {
        "id": "5a4d93bb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Plot n_estimators vs R² score\n# Convert GridSearchCV results to a DataFrame\ncv_results = pd.DataFrame(grid_search_rf.cv_results_)\n\n# Convert hyperparameters to integer type for plotting\ncv_results[\"param_n_estimators\"] = cv_results[\"param_n_estimators\"].astype(int)\ncv_results[\"param_max_depth\"] = cv_results[\"param_max_depth\"].astype(int)\n\n# Plot n_estimators vs Mean Test R² Score\nplt.figure(figsize=(8, 5))\nsns.lineplot(x=cv_results[\"param_n_estimators\"], y=cv_results[\"mean_test_score\"], marker=\"o\")\nplt.title(\"Effect of n_estimators on Performance\")\nplt.xlabel(\"Number of Estimators\")\nplt.ylabel(\"Mean Test R² Score\")\nplt.grid()\nplt.show()",
      "metadata": {
        "id": "iPpVLLBrvooU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "from sklearn.metrics import mean_squared_error, r2_score\n\nbest_model = grid_search_rf.best_estimator_\n\ny_pred = best_model.predict(X_test_scaled)\nprint(\"Test Set R² Score:\", r2_score(y_test, y_pred))\nprint(\"Test Set MSE:\", mean_squared_error(y_test, y_pred))",
      "metadata": {
        "id": "NeV-HTRVsQAX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "feature_importance = pd.DataFrame({\n    'Feature': X_train.columns,\n    'Importance': best_model.feature_importances_\n}).sort_values(by=\"Importance\", ascending=False)\n\nprint(feature_importance)",
      "metadata": {
        "id": "Cd5t4VBmqpt8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Keep only the top 2 features\ntop_features = ['trip_distance_km', 'avg_speed_kmh']\n\n# Convert scaled arrays back to DataFrames\nX_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\nX_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n\n# Create new datasets with only these features\nX_train_selected = X_train_scaled_df[top_features]\nX_test_selected = X_test_scaled_df[top_features]",
      "metadata": {
        "id": "c6nXtfawt0XN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "best_model.fit(X_train_selected, y_train)\n\n# Evaluate performance\ny_pred = best_model.predict(X_test_selected)\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"MSE: {mse:.4f}, R² Score: {r2:.4f}\")",
      "metadata": {
        "id": "i5HNdSqtt0Tx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Conclusion\n\nIn this project, I explored the NYC Taxi Trip Duration dataset to build a predictive model for trip duration. Below are the key steps taken:\n\n1. **Data Preprocessing:**  \n   - Loaded and examined the dataset.  \n   - Handled missing values and outliers.  \n   - Created new features such as `trip_distance_km` and `avg_speed_kmh`.  \n   - Scaled numerical features for better model performance.  \n\n2. **Feature Selection:**  \n   - Performed feature importance analysis using Random Forest.  \n   - Identified `trip_distance_km` and `avg_speed_kmh` as the most significant features.  \n   - Reduced the dataset to only the most relevant features.  \n\n3. **Model Training & Evaluation:**  \n   - Compared multiple models (Linear Regression, Decision Tree, and Random Forest).  \n   - Used **cross-validation** to evaluate model performance.  \n   - Fine-tuned the Random Forest model using **GridSearchCV** to optimize hyperparameters.  \n\n4. **Results & Insights:**  \n   - Random Forest outperformed other models with better R² scores.  \n   - Visualized hyperparameter tuning results to understand model behavior.  \n   - The final model effectively predicts trip duration based on key trip features.  \n\nThis project demonstrated the end-to-end machine learning workflow, from **data exploration** to **model optimization**, providing valuable insights into taxi trip durations. 🚖📊  \n\n---\n",
      "metadata": {
        "id": "QQpD9iOewqFL"
      }
    }
  ]
}